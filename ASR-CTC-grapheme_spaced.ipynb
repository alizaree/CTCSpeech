{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "from hdf5storage import loadmat, savemat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from api.preprocessing import sound2coch\n",
    "from api.librispeech import LibriDataset\n",
    "from api.model import SpeechRecognitionCTC\n",
    "\n",
    "# CUDA for PyTorch\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency dimension: 65, alphabet size: 42\n"
     ]
    }
   ],
   "source": [
    "LibriDataset.set_mode('phoneme') # Should be one of 'grapheme', 'phoneme', 'word'\n",
    "LibriDataset.set_spacing(True)\n",
    "\n",
    "freq_bins = 65\n",
    "size_vocab = LibriDataset.vocab_size()\n",
    "print(f'Frequency dimension: {freq_bins}, alphabet size: {size_vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phoneme-spaced-3-500\n"
     ]
    }
   ],
   "source": [
    "N_LAYER, N_NODES = 3, 500\n",
    "model_id = f'{LibriDataset.MODE}-spaced-{N_LAYER}-{N_NODES}'\n",
    "model_arch = dict(rnn_hidden_size=N_NODES, nb_layers=N_LAYER, window_size=10, rnn_stride=2)\n",
    "model_name = 'models/model-ctc-{:s}'.format(model_id)\n",
    "\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpeechRecognitionCTC(rnn_type=nn.GRU, labels=LibriDataset.alphabet(), **model_arch, freq_bins=freq_bins)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'{model_name}.pt', map_location=device))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IH Z AH B EH L <SPACE> P R ER Z AH N T L IY <SPACE> F AW N D <SPACE> HH ER S EH L F <SPACE> IH N <SPACE> DH AH <SPACE> S IH NG G Y AH L ER <SPACE> S IH CH UW EY SH AH N <SPACE> AH V <SPACE> D IH F V EH N D IH NG <SPACE> DH AH <SPACE> B R IH T IH SH <SPACE> K AA N S T AH T UW SH AH N <SPACE> AH G EH N S T <SPACE> HH ER <SPACE> AE N T\n"
     ]
    }
   ],
   "source": [
    "alphabet = LibriDataset.alphabet()\n",
    "sound_file = '/archive/menoua/Data/LibriSpeech/LibriSpeech/train-clean-100/1098/133695/1098-133695-0005.flac'\n",
    "cochleagram = sound2coch(sound_file)\n",
    "\n",
    "pred = model.get_prediction(cochleagram)\n",
    "actv = model.get_activation(cochleagram)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom inferrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans = []\n",
    "# for layer in range(nb_layers):\n",
    "#     svd = sklearn.decomposition.TruncatedSVD(n_components=65, n_iter=25)\n",
    "#     svd.fit(acts_libri[layer])\n",
    "#     trans.append(svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IH Z AH B EH L <SPACE> P R ER Z AH N T L IY <SPACE> F AW N D <SPACE> HH ER S EH L F <SPACE> IH N <SPACE> DH AH <SPACE> S IH NG G Y AH L ER <SPACE> S IH CH UW EY SH AH N <SPACE> AH V <SPACE> D IH F V EH N D IH NG <SPACE> DH AH <SPACE> B R IH T IH SH <SPACE> K AA N S T AH T UW SH AH N <SPACE> AH G EH N S T <SPACE> HH ER <SPACE> AE N T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphabet = LibriDataset.alphabet()\n",
    "sound_file = '/archive/menoua/Data/LibriSpeech/LibriSpeech/train-clean-100/1098/133695/1098-133695-0005.flac'\n",
    "cochleagram = sound2coch(sound_file)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    xs = torch.Tensor(cochleagram).unsqueeze(0)\n",
    "    xs = xs.type(torch.float32).to(device)\n",
    "    xlen = torch.LongTensor([xs.shape[1]])\n",
    "    z, zlen = model.predict(xs, xlen)\n",
    "    \n",
    "    zi = [_ for _ in z.squeeze().argmax(dim=1)]\n",
    "    for i in range(len(zi)-1,0,-1):\n",
    "        if zi[i] == zi[i-1]: zi = zi[:i-1] + zi[i:]\n",
    "    zi = [alphabet[_] for _ in zi if _ > 0]\n",
    "    \n",
    "    zi = ['<SPACE>' if _ == LibriDataset.SYM_SPACE else _ for _ in zi]\n",
    "    zi = ' '.join(zi)\n",
    "    \n",
    "    print(zi)\n",
    "    print()\n",
    "    \n",
    "    activation = model.activations(xs)\n",
    "    activation = [x.squeeze(0).numpy().astype('float32') for x in activation]\n",
    "    # activation = [trans[layer].transform(activation[layer]) for layer in range(nb_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348, 500) (348, 500) (348, 500)\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([str(x.shape) for x in activation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savemat(f'activations-ctc-{model_id}-100hz-20ms.mat', mdict={'acts_neural': acts_neural, 'stim_neural': stim_neural})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
